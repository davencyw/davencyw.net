<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cutting Through the Noise: Automating AI Paper Review with Language Models</title>
    <link rel="stylesheet" href="https://latex.vercel.app/style.css">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- syntax highlighting -->
    <link rel="stylesheet" href="https://latex.vercel.app/prism/prism.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-python.min.js"></script>

    <style>
        #dark-mode-toggle {
            position: fixed;
            bottom: 30px;
            right: 30px;
            padding: 10px 15px;
            background-color: #e0e0e0;
            color: #333;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            z-index: 1000;
        }

        .banner {
            background-color: #fff8dc;
            color: #000;
            padding: 10px 20px;
            text-align: center;
            font-weight: bold;
            border-bottom: 2px solid #ffd700;
        }

        .full-width-banner {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            background-color: #fff8dc;
            color: #000;
            padding: 10px 0;
            text-align: center;
            font-weight: bold;
            border-bottom: 2px solid #ffd700;
            z-index: 1000;
        }

        body:has(.full-width-banner) header {
            margin-top: 60px;
        }

        body:has(.full-width-banner) #dark-mode-toggle {
            margin-top: 60px;
        }
    </style>
</head>



<body class="latex-light">
    <div class="full-width-banner">
        This article is a <i>Work in Progress</i>. Content may be incomplete or subject to change.
    </div>


    <button id="dark-mode-toggle">Dark Mode</button>
    <script>
        const toggleButton = document.querySelector("#dark-mode-toggle");
        toggleButton
            .addEventListener('click', () => {
                document.body.classList.toggle("latex-dark");
                console.log(toggleButton.textContent);
                if (toggleButton.textContent == "Light Mode") {
                    toggleButton.textContent = "Dark Mode";
                    toggleButton.style.backgroundColor = "#e0e0e0"; // Light background
                    toggleButton.style.color = "#333"; // Dark text
                } else {
                    toggleButton.textContent = "Light Mode";
                    toggleButton.style.backgroundColor = "#565656"; // Dark background
                    toggleButton.style.color = "white"; // Light text
                }
            });
    </script>

    <header>
        <h1>Cutting Through the Noise: Automated AI Paper Curation with Language Models</h1>
    </header>


    <p class="author">David Schmidig <br> January 7, 2025</p>

    <div class="abstract">
        <h2>Abstract</h2>
        <p>
            The rapid growth of research publications in Computer
            <label for="sn-1" class="sidenote-toggle sidenote-number">Vision</label>
            <input type="checkbox" id="sn-1" class="sidenote-toggle" />
            <span class="sidenote">In 2024, there were more than 20'000 submitted papers to ECCV and CVPR only. Slightly
                over
                5'000 papers were accepted to both conferences combined, making it impossible for a single person to
                keep up
                with all the research that is happening.</span>
            and Artificial Intelligence makes staying
            updated a daunting task. To address this, I developed a tool that automates the discovery and filtering of
            research papers from sources like
            <label for="sn-1" class="sidenote-toggle sidenote-number">Arxiv</label>
            <input type="checkbox" id="sn-1" class="sidenote-toggle" />
            <span class="sidenote">
                See full list of possible sources in Apendix A-I.
            </span>
            . This tool leverages fine-tuned language models, human feedback, and
            keyword relevance to streamline paper curation, ensuring that only the most relevant works reach the curious
            user. This article outlines the technical process behind building the tool, including data ingestion, model
            fine-tuning, and feedback integration, offering insights into how AI can simplify the research workflow.
        </p><br>
    </div>


    <main>

        <article>
            <figure>
                <img src="/davencyw.net/projects/paperviewer/overview.png">
                <figcaption>Overview of the whole system</figcaption>
            </figure>

            <h2>Motivation</h2>
            <p>
                The flood of daily publications in Computer Vision and AI poses a challenge for anyone wanting to
                consume
                research:
                identifying valuable insights without spending hours sifting through papers. Traditional
                keyword or author searches often miss nuanced context or fail to adapt to evolving interests and can
                miss
                relevant papers.
                <label for="sn-1" class="sidenote-toggle sidenote-number">Tools</label>
                <input type="checkbox" id="sn-1" class="sidenote-toggle" />
                <span class="sidenote">
                    See full list in Appendix A-II
                </span>
                such as litmap or connected papers fall short if it's about the most recent research.
                I aimed to solve this by creating a system that not only automates the selection process
                but also learns from user preferred topics. By combining AI-driven filtering with human feedback loops,
                this
                tool empowers a user to focus on innovation rather than information overload.
            <h2>Approach</h2>
            <h3>Overview</h3>
            The system consists of four main components (as seen in Fig. 1):
            <ul>
                <li>Paper Database</li>
                <li>Paper Scraper</li>
                <li>Learned Paper Curator</li>
                <li>User Facing Application</li>
            </ul>
            In the following paragraph, each of those parts whill be
            detailed followed by more detailed insights into
            the sources, scoring and learning details.
            </p>

            <p><br><b>Paper Database</b><br>
                Such that the whole process is independent from any external sources, I decided to have my own paper
                database which stores the needed paper meta-information as well as a link to the pdf. This allows for
                easy ingestion of new papers as well as checking if the same paper has already been ingested, if it
                appears in multiple sources. Lastly, it allows for custom meta data which is important for tagging and
                training.<br>
                The paper metadata stored in the database, which is also responsible for the database definition, looks
                like this:
            </p>
            <pre>
                <code class="language-python">
    @dataclass
    class Paper:
        title: str
        authors: List[str]
        conferences: Optional[str]
        comment: str
        abstract: str
        categories: List[str]
        paper_id: str
        pdf_url: str
        pdf_file: Optional[str]
        published: datetime
        source: str
        score: Optional[float]
        selected_review: Optional[bool]
        selected_review_date: Optional[datetime]
            </code>
            </pre>

            <p>
                <br><b>Paper Scraper</b><br>
                The paper scraper is responsible for finding and ingesting all new research publications from different
                sources online. The scraper has been designed to be as flexible as possible allowing all possible
                sources to be ingested. For now, only sources with a clean Python-API have been incorporated, namely
                Arxiv and Google Scholar. The scraper runs independently of the curator or the user application and
                simply updates the database periodically with new papers.<br>

                <br><b>Learned Paper Curator</b><br>
                The core element of the whole system is by far the learned paper curator. This part is responsible to
                score papers based on their metadata such that only relevant papers will make it to the user and noise
                is being cut away. As simple as it sounds, _paper meta-data_ goes in, a _relevancy score_ gets returned.
                Scoring happens through a finetuned language model that classifies metadata and is detailed in section
                Scoring below. The scorer is written in Python (PyTorch for the model part). The curator is in interplay
                with the user (through the application) to ingest user decisions on relevancy into the learned model
                again.
                <br>
                <br><b>User Facing Application</b><br>
                Lastly, the whole application has to have a user-facing frontend that allows a user to inspect and open
                papers as well as re-classify them depending on their relevancy to the user. This feedback is then again
                incorporated into the learned paper curator. The application is built with Flask and has a simple
                web-interface that allows for all necessarily UI elements while connecting to the curator and the
                database through Python.
                <br>
            </p>

            <h3>Sources</h3>
            <h3>Scoring</h3>
            <p>
                The crucial part of the efficiency and value of the system lies in the accurate relevancy scoring of the
                papers. Obviosly, this depends solely on the interests of the user. Luckily for me, I used a slightly
                simpler system for a bit more than a year which was only using key-word and author based
                <label for="sn-1" class="sidenote-toggle sidenote-number">scoring</label>
                <input type="checkbox" id="sn-1" class="sidenote-toggle" />
                <span class="sidenote">
                    For this, I used a dict of word-value pairs. If any of those words appeared in the title the value
                    of said word is added to the score.
                </span>
                . This gave me enough data (albeit heavily imbalanced, but more about that later) to experiment with
                a learned approach.
            </p>

            <h4>Model Finetuning</h4>
            <p>
                I decided to go for a straight forward language based paper classification which uses a combination of
                title, abstract, author and conferences. The model of choice is <code>ModernBERT</code> for binary
                text classification. The input to the tokenizer is a concatenation of title, authors and abstract:
                <code>text = f"{title} | {authors} | {abstract}"</code>. The max input length is 1024 characters which
                is quite small, thus the abstract has been stripped of all stop words.
                Since the dataset is heavily imbalanced due to the
                majority of the papers not being relevant, I applied undersampling the negative class and adapted the
                class weights for the loss computation. Given roughly 800 positives and 40'000 negative samples, the
                precision/recall and ROC curve can be found below. It's important to notice that a relatively high
                recall for positive samples is desired while a lower precision is not as severe. It's the opposite for
                negatives samples.
                The results for the initial finetuning can be seen in Fig 2. and Fig. 3.
            </p>

            <h4>Reinforcement Learning Updates</h4>
            <p>
                Once the user is presented with selected
                <label for="sn-1" class="sidenote-toggle sidenote-number">papers</label>
                <input type="checkbox" id="sn-1" class="sidenote-toggle" />
                <span class="sidenote">
                    Note that it's crucial to not skip relevant papers and thus the conditions for relevant papers are
                    periodically relaxed to manually review more data. This is to avoid a collapse on predicted data.
                </span>
                , the learned model can be updated with new information. Since the actual task is well defined with
                binary labels and the imbalance of the data can be addressed with class weights, focal loss or sampling
                one can argue that finetuning a model should be sufficient. To address the continuous data stream we
                also could simply continue to finetune or use continual learning. However, the main problem with this
                approach is that the interest of a user might change over time. This is due to the persons interest,
                professional occupation or due to technological landsacpe
                <label for="sn-1" class="sidenote-toggle sidenote-number">changing</label>
                <input type="checkbox" id="sn-1" class="sidenote-toggle" />
                <span class="sidenote">
                    There are multiple example just in the past year. Moving away from GANs, model scaling or federated
                    learning and moving towards multi-modality, foundation models or model alignment.
                </span>
                . To address those challenges, I decided to employ a RLHF mechanism on top of the initial finetuning.
                I implemented the RLHF meachnism using OpenRLHF <sup><a href="#fn1" id="ref1">1</a></sup>.
            </p>
            <h3>Interface</h3>
            <p>
                The interface consists of a simple UI that has two different pieces: the overview and the review page.
                The overview consists of all reviewed and selected papers grouped by
                <label for="sn-1" class="sidenote-toggle sidenote-number">week</label>
                <input type="checkbox" id="sn-1" class="sidenote-toggle" />
                <span class="sidenote">
                    I made the arbitrary choice to have weekly cycles to aggregate papers. That gives
                    a nice tradeoff of recency and digestable quantity.
                </span>
                along with some metadata.<br>
            </p>

            <p>
                <br><b>Overview Page</b><br>
                The central element of the overview page is the week selector, which lists all relevant papers for a
                given
                week. Clicking on one of the papers shows the abstract as well as as some meta-information and a direct
                link
                to the PDF on disk to read and annotate.
            <figure>
                <img src="/davencyw.net/projects/paperviewer/overview_page.png">
                <figcaption>Overview page</figcaption>
            </figure>
            </p>

            <p>
                <br><b>Review Page</b><br>
                The review page consists of centrally located navigation as well as paper selectors (ignore or keep)
                which
                can also be accessed through hotkeys. There are metadata information about the current paper on the left
                side as well as all currently selected papers from this week. In the middle, the paper in question is
                presented and
                <label for="sn-1" class="sidenote-toggle sidenote-number">loaded</label>
                <input type="checkbox" id="sn-1" class="sidenote-toggle" />
                <span class="sidenote">
                    The application caches PDFs on disk to ensure fast loading when switching papers.
                </span>
                directly as an iframe.
            <figure>
                <img src="/davencyw.net/projects/paperviewer/review_page.png">
                <figcaption>Review page</figcaption>
            </figure><br>
            </p>

            <h2>Conclusion</h2>
            <h2>Limitations and Future Work</h2>
            annotation
            scraping the web
            automatic summary generation, put in context
            write code and experiment, combine papers

            <div class="footnotes">
                <p id="fn1">
                    1. <a href="https://github.com/OpenRLHF/OpenRLHF">https://github.com/OpenRLHF/OpenRLHF</a>.
                    <a href="#ref1" title="OpenRLHF.">â†©</a>
                </p>
            </div>
        </article>
        <article>
            <br>
            <br>
            <hr>
            <br>
            <h2>Appendix A</h2>
            <h3>I - Research Paper Resources</h3>
            <p>
                Incomplete list of paper sources.
            <ul>
                <li>Arxiv</li>
                <li>Google Scholar</li>
                <li>BASE</li>
                <li>Papers With Code</li>
                <li>Hugging Face Spaces</li>
                <li>Twitter/X</li>
            </ul>
            <h3>II - Paper Tools</h3>
            <p>
                Incomplete list of research tools.
            <ul>
                <li>Semantic Scholar</li>
                <li>Influence Map</li>
                <li>Connected Papers</li>
                <li>Litmaps</li>
            </ul>

            </p>
        </article>
    </main>
</body>